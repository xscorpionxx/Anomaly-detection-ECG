{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wyn1CzqOzUK4"
   },
   "outputs": [],
   "source": [
    "arff.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3RY_N3gOmfDi",
    "outputId": "a0f7949a-c097-4bbc-f391-d7649e9b0aa4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import arff\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "#from arff2pandas import a2p\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RN0e4KktjE88"
   },
   "source": [
    "In this tutorial, you'll learn how to detect anomalies in Time Series data using an LSTM Autoencoder. You're going to use real-world ECG data from a single patient with heart disease to detect abnormal hearbeats.\n",
    "\n",
    "- [Read the tutorial](https://www.curiousily.com/posts/time-series-anomaly-detection-using-lstm-autoencoder-with-pytorch-in-python/)\n",
    "- [Run the notebook in your browser (Google Colab)](https://colab.research.google.com/drive/1_J2MrBSvsJfOcVmYAN2-WSp36BtsFZCa)\n",
    "- [Read the Getting Things Done with Pytorch book](https://github.com/curiousily/Getting-Things-Done-with-Pytorch)\n",
    "\n",
    "By the end of this tutorial, you'll learn how to:\n",
    "\n",
    "- Prepare a dataset for Anomaly Detection from Time Series Data\n",
    "- Build an LSTM Autoencoder with PyTorch\n",
    "- Train and evaluate your model\n",
    "- Choose a threshold for anomaly detection\n",
    "- Classify unseen examples as normal or anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9leLfjjlHVE"
   },
   "outputs": [],
   "source": [
    "#########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnW3JsIFW_Yw"
   },
   "source": [
    "## 1-  Get the Data\n",
    "\n",
    "\n",
    "\n",
    "The [dataset](http://timeseriesclassification.com/description.php?Dataset=ECG5000) contains 5,000 Time Series examples (obtained with ECG) with 140 timesteps. Each sequence corresponds to a single heartbeat from a single patient with congestive heart failure.\n",
    "\n",
    "> An electrocardiogram (ECG or EKG) is a test that checks how your heart is functioning by measuring the electrical activity of the heart. With each heart beat, an electrical impulse (or wave) travels through your heart. This wave causes the muscle to squeeze and pump blood from the heart. [Source](https://www.heartandstroke.ca/heart/tests/electrocardiogram)\n",
    "\n",
    "We have 5 types of hearbeats (classes):\n",
    "\n",
    "- Normal (N) \n",
    "- R-on-T Premature Ventricular Contraction (R-on-T PVC)\n",
    "- Premature Ventricular Contraction (PVC)\n",
    "- Supra-ventricular Premature or Ectopic Beat (SP or EB) \n",
    "- Unclassified Beat (UB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDlfeY2VAYdU",
    "outputId": "adede0d3-fb26-4850-eac6-360b76d75207"
   },
   "outputs": [],
   "source": [
    "!gdown --id 16MIleqoIr1vYxlGk4GKnGmrsCPuWkkpT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_gYlNi2AaOK"
   },
   "outputs": [],
   "source": [
    "!unzip -qq ECG5000.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eEYusNcsmWOC"
   },
   "outputs": [],
   "source": [
    "! pip install arff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVvmnVJJl2Qi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import arff\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EngRM6LvmaRC"
   },
   "source": [
    "The data comes in multiple formats. We'll load the `arff` files into Pandas data frames:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDtaZ2uTCG11"
   },
   "source": [
    "We'll combine the training and test data into a single data frame. This will give us more data to train our Autoencoder. We'll also shuffle it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D8B2sU4V3NrD"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.io.arff import loadarff \n",
    "\n",
    "raw_data = loadarff('/content/ECG5000_TRAIN.arff')\n",
    "df_data = pd.DataFrame(raw_data[0])\n",
    "raw_data_test = loadarff('/content/ECG5000_TEST.arff')\n",
    "df_data_test = pd.DataFrame(raw_data_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dIc36z8851Di"
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df_data ,df_data_test ] ,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "5-kkWDC96IuZ",
    "outputId": "003f2efb-c598-4cda-ea21-b62c2f2bc35b"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_f7F-ipCZNH"
   },
   "source": [
    "We have 5,000 examples. Each row represents a single heartbeat record. Let's name the possible classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fcKHB5rcDcm"
   },
   "outputs": [],
   "source": [
    "CLASS_NORMAL = 1\n",
    "\n",
    "class_names = ['Normal','R on T','PVC','SP','UB']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlf5aVLnjRCz"
   },
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Let's check how many examples for each heartbeat class do we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oyy7l_cS8Yor"
   },
   "outputs": [],
   "source": [
    "df.target =  df.target.apply(lambda x : str(x)[2] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QBxuqD1cdd2y",
    "outputId": "e8f555e1-e746-44b4-b413-9f7d71700e66"
   },
   "outputs": [],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EZVZYCsQKXV"
   },
   "source": [
    "Let's plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "wn4s427Sf4eb",
    "outputId": "21895ab1-c48b-49f0-9728-90d54e8d44c0"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ax = sns.countplot(df.target)\n",
    "ax.set_xticklabels(class_names);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2m55fTyRNIf"
   },
   "source": [
    "The normal class, has by far, the most examples. This is great because we'll use it to train our model.\n",
    "\n",
    "Let's have a look at an averaged (smoothed out with one standard deviation on top and bottom of it) Time Series for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVvG0vhiU-ju"
   },
   "outputs": [],
   "source": [
    "def plot_time_series_class(data, class_name, ax, n_steps=10):\n",
    "  time_series_df = pd.DataFrame(data)\n",
    "\n",
    "  smooth_path = time_series_df.rolling(n_steps).mean()\n",
    "  path_deviation = 2 * time_series_df.rolling(n_steps).std()\n",
    "\n",
    "  under_line = (smooth_path - path_deviation)[0]\n",
    "  over_line = (smooth_path + path_deviation)[0]\n",
    "\n",
    "  ax.plot(smooth_path, linewidth=2)\n",
    "  ax.fill_between(\n",
    "    path_deviation.index,\n",
    "    under_line,\n",
    "    over_line,\n",
    "    alpha=.125\n",
    "  )\n",
    "  ax.set_title(class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 579
    },
    "id": "xHaslHZ8JMSk",
    "outputId": "cb0b1177-4a09-43e2-db5e-b32669750ef7"
   },
   "outputs": [],
   "source": [
    "classes = df.target.unique()\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "  nrows=len(classes) // 3 + 1,\n",
    "  ncols=3,\n",
    "  sharey=True,\n",
    "  figsize=(14, 8)\n",
    ")\n",
    "\n",
    "for i, cls in enumerate(classes):\n",
    "  ax = axs.flat[i]\n",
    "  data = df[df.target == cls] \\\n",
    "    .drop(labels='target', axis=1) \\\n",
    "    .mean(axis=0) \\\n",
    "    .to_numpy()\n",
    "  plot_time_series_class(data, class_names[i], ax)\n",
    "\n",
    "fig.delaxes(axs.flat[-1])\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDt33QoDTUIb"
   },
   "source": [
    "It is very good that the normal class has a distinctly different pattern than all other classes. Maybe our model will be able to detect anomalies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGKKj6fgUV_a"
   },
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Let's get all normal heartbeats and drop the target (class) column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "juvktiHsDCNA"
   },
   "outputs": [],
   "source": [
    "# we need to shuffle the data before\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NA0k8mTijyh-",
    "outputId": "31284258-47b9-4446-8a40-86c73ef6cda5"
   },
   "outputs": [],
   "source": [
    "normal_df = df[df.target == str(CLASS_NORMAL)].drop(labels='target', axis=1)\n",
    "normal_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRCZ7uviaI1Y"
   },
   "source": [
    "We'll merge all other classes and mark them as anomalies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xpdXIaDJstD3",
    "outputId": "7c0264d4-10d7-48af-8598-91ca4e80cf80"
   },
   "outputs": [],
   "source": [
    "anomaly_df = df[df.target != str(CLASS_NORMAL)].drop(labels='target', axis=1)\n",
    "anomaly_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILcJJwpda15z"
   },
   "source": [
    "We'll split the normal examples into train, validation and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n7kJ7C3IFWIV"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(\n",
    "  normal_df,\n",
    "  test_size=0.15,\n",
    "  random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "  val_df,\n",
    "  test_size=0.33, \n",
    "  random_state=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWasRQ1dcAp4"
   },
   "source": [
    "We need to convert our examples into sequences, so we can use them to train our Autoencoder. Let's write a helper function for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2kKiIIeBwKb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def create_dataset(df):\n",
    "\n",
    "  sequences = np.array(df.astype(np.float32).to_numpy().tolist())\n",
    "\n",
    "  sequences = np.expand_dims(sequences , axis=2)\n",
    "  return sequences #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rb1UeUwbjmMD"
   },
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(train_df)\n",
    "val_dataset= create_dataset(val_df)\n",
    "test_normal_dataset = create_dataset(test_df)\n",
    "test_anomaly_dataset = create_dataset(anomaly_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t1nB0gljGQv4",
    "outputId": "10099bc0-aba4-48c9-e198-23d74d1d0149"
   },
   "outputs": [],
   "source": [
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SzKOXvwqR6w"
   },
   "source": [
    "# Build LSTM autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nzik9sZFqY4D"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Input, Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qyw4-10UWfWh",
    "outputId": "ac565390-fde2-41cf-bee2-0c743709eb6e"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(train_dataset.shape[1], train_dataset.shape[2])))\n",
    "model.add(Dropout(rate=0.2))\n",
    "\n",
    "model.add(RepeatVector(train_dataset.shape[1]))\n",
    "\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(TimeDistributed(Dense(train_dataset.shape[2])))\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "INcKsMvhdS5e",
    "outputId": "561aae34-2ca8-4a34-eb0c-b6140862ca57"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, train_dataset, epochs=10, batch_size=32, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "m4IdW_dzzFrs",
    "outputId": "83d9d20e-d83f-41b7-ad8b-6d8550764cfc"
   },
   "outputs": [],
   "source": [
    "epochs = [i for i in range(10)]\n",
    "plt.plot(epochs , history.history['loss'] , 'g-o' , label = 'Training Loss')\n",
    "plt.plot(epochs , history.history['val_loss'] , 'r-o' , label = 'Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnAKGNVA0jFL"
   },
   "source": [
    "# compute recounstruction loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GrOjq_EDqUA1"
   },
   "outputs": [],
   "source": [
    "def sequence_prediction(data ):\n",
    "  pre = model.predict(data )\n",
    "  trainMAE = np.mean(np.abs(pre - data), axis=1)\n",
    "  return trainMAE    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3OA2iS-br2Dq"
   },
   "outputs": [],
   "source": [
    "preictions_normal_losses = sequence_prediction(test_normal_dataset )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "4ZfyOTB214kt",
    "outputId": "4d549621-ab16-4b7c-8eb8-89988d969237"
   },
   "outputs": [],
   "source": [
    "sns.distplot(preictions_normal_losses, bins=50, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOuEle3h2Af1"
   },
   "outputs": [],
   "source": [
    "def sequence_prediction_(data , threshold):\n",
    "  outputs = []\n",
    "  pre = model.predict(data )\n",
    "  trainMAE = np.mean(np.abs(pre - data), axis=1)\n",
    "  for i in range(len(trainMAE)) :\n",
    "    if trainMAE[i] > threshold : \n",
    "      outputs.append('anomaly')\n",
    "    else : \n",
    "      outputs.append('normal')\n",
    "  return outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fagBudD52vOK"
   },
   "outputs": [],
   "source": [
    "preictions_normal = sequence_prediction_(test_normal_dataset , 0.4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uV7xIaZVsmN0",
    "outputId": "946006c2-00e4-4697-8e1b-3551f5920b07"
   },
   "outputs": [],
   "source": [
    "preictions_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANfG3nXN0rgm"
   },
   "outputs": [],
   "source": [
    "preictions_anomaly = sequence_prediction_(test_anomaly_dataset , 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sDVr9BNr0zUF",
    "outputId": "11fb2dc7-7eb7-45e0-f5f9-60e1de7d65de"
   },
   "outputs": [],
   "source": [
    "preictions_anomaly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2IGZFBEVJjq"
   },
   "source": [
    "Our model converged quite well. Seems like we might've needed a larger validation set to smoothen the results, but that'll do for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmoaNSERn09J"
   },
   "source": [
    "## Saving the model\n",
    "\n",
    "Let's store the model for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLC_ClIpnv9H",
    "outputId": "761a328c-1fb8-4538-b8d2-a17c3bcc5bd3"
   },
   "outputs": [],
   "source": [
    "model.save('/content/anomaly_detection.h5')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
